---
title: "Spotify Wrapped"
author: "Julie Deckers"
date: "2/13/2021"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
   

---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(message=FALSE, echo = FALSE)
```

```{r}
library(tidyverse)
library(compmus)
library(ggplot2)
library(spotifyr)
library(plotly)
library(Cairo)
library(dplyr)
library(ggdendro)
library(heatmaply)


library(tidymodels)

Julie <- get_playlist_audio_features("","37i9dQZF1EM27HE4pdYYcz") 
Hemke <- get_playlist_audio_features("","37i9dQZF1EMgeVFiZwhb4Z")
Fleur <- get_playlist_audio_features("","37i9dQZF1EMd6LXIgrTylx")

Spotify_Wrapped <-
  bind_rows(
    Julie %>% mutate(category = "Julie"),
    Hemke %>% mutate(category = "Hemke"),
    Fleur %>% mutate(category = "Fleur")
      )
```



### **Comparing** Three Spotify Wrapped Playlists
For my corpus I would like to compare my Spotify Wrapped playlist to my friends and familys Wrapped playlists. The reason I chose this is because I think that a lot of people like the Wrapped playlists and its a fun way to see what you listened to the previous year. I also think it's interesting to see if the playlists are accurate and if the persons actually listened to the songs in it. I think it's fun to just see what I listened to and what my friends listend to, and what the difference is. 

I have 2 Wrapped playlists to compare to mine. To compare the three playlist, I'm going to use the Spotify API features. I think the most important features to look at for my corpus are valence, danceability and energy. I know mine has a lot of pop music in it, so I expect it will have a lot of songs in it with a fast tempo and a high valence. One of my friends, Hemke, listened to a few artists that i also listened to, so i expect ours will have some similar features. Those features will probably be danceablilty and high valence.  My other friend, Fleur, listens to more rap music, so i expect hers to have a lot of energy and maybe danceability. Fleur has a lot of songs in her playlist that I dont know, so i think it will be interesting to see how it compares to mine and Hemke's playlists. I expect it will be quite different to ours. We all listen to music only in Spotify, so I think the information that i get from the playlists will be accurate.

The playlist that I'm going to use are: 
 - [Your Top Songs 2020 Fleur](https://open.spotify.com/playlist/37i9dQZF1EMd6LXIgrTylx?si=RKx64nfqQvSu9u5nVB8Djw)
 -[Your Top Songs 2020 Julie](https://open.spotify.com/playlist/37i9dQZF1EM27HE4pdYYcz?si=3s50W2rVQZCg0QxQfJ2l0Q)
 - [Your Top Songs 2020 Hemke](https://open.spotify.com/playlist/37i9dQZF1EMgeVFiZwhb4Z?si=oAuexJI6SqqlI7AM82IKvA)



### Looking at the **playlists** one by one
#### Fleur likes to dance 
```{r}

library(tidyverse)
library(ggplot2)
library(spotifyr)
library(plotly)
library(Cairo)
library(dplyr)

Julie <- get_playlist_audio_features("","37i9dQZF1EM27HE4pdYYcz") 
Hemke <- get_playlist_audio_features("","37i9dQZF1EMgeVFiZwhb4Z")
Fleur <- get_playlist_audio_features("","37i9dQZF1EMd6LXIgrTylx")

Spotify_Wrapped <-
  bind_rows(
    Julie %>% mutate(category = "Julie"),
    Hemke %>% mutate(category = "Hemke"),
    Fleur %>% mutate(category = "Fleur")
      )
```

```{r}
plotf<-ggplot(Fleur,aes(x=valence,y=danceability,size =energy,label=track.name,alpha=0.5)) + geom_point() +ggtitle("Fleur") + geom_smooth() + expand_limits(x = 0, y = 0)

ggplotly(plotf)


```

****
As you can see in this scatterplot Fleur's playlist has a lot danceability, so that matches with my previous statement that Fleur listens to a lot of hiphop. Her playlist also has a decent amount of energy. There is only one song with low energy, namely "i love you"by Billie Eilish. 


#### Julie, from low to high

```{r}
plotj <-ggplot(Julie,aes(x=valence,y=danceability,size=energy, label=track.name, alpha=0.5)) + geom_point() +ggtitle("Julie") + geom_smooth() + expand_limits(x = 0, y = 0)

ggplotly(plotj)
```
**** 
My playlist is a bit of a mix. I have songs that have low energy, but also high. The same goes for valence and danceability. If you compare my danceability with fleur's, it is a lot lower than Fleur's. 


#### Hemke is the middle ground

```{r}
ploth<-ggplot(Hemke,aes(x=valence,y=danceability,size=energy,label=track.name,alpha=0.5)) + geom_point() +ggtitle("Hemke") + geom_smooth() +expand_limits(x = 0, y = 0)

ggplotly(ploth)
```
****
Hemke's danceablity level is centered in the middle erea in the plot. This also counts for the valence and energy. It's very straight forward, in the best way of course, with a few outliers.


### How does it look if the *playlist* are in **one plot**?

```{r}
library(tidyverse)
library(ggplot2)
library(spotifyr)
library(plotly)
library(Cairo)
library(dplyr)

Julie <- get_playlist_audio_features("","37i9dQZF1EM27HE4pdYYcz") 
Hemke <- get_playlist_audio_features("","37i9dQZF1EMgeVFiZwhb4Z")
Fleur <- get_playlist_audio_features("","37i9dQZF1EMd6LXIgrTylx")

Spotify_Wrapped <-
  bind_rows(
    Julie %>% mutate(category = "Julie"),
    Hemke %>% mutate(category = "Hemke"),
    Fleur %>% mutate(category = "Fleur")
      )




```






```{r}
plot1 <- ggplot(Spotify_Wrapped,aes(x=valence,y=danceability,color=category,size=energy, alpha=0.5,label=track.name)) + geom_point() +ggtitle("All three playlists together") + geom_smooth() 

ggplotly(plot1)


```

***
In this plot you can better see that Fleur's playlist has more dancebaility than my and Hemke's playlist. You can also see that my and hemke's playlist lie very close together, but that mine has a bit less danceability and energy. The features were very similar to what I expected. I didn't really expect that mine would have as many songs with low energy, valence and danceability as it has. I'll go one by one through the outliers of all three of the playlists. 

I'll start with my playlist. The songs with the lowest danceability (under 0.3) are "Once Upon a December", "All is Found", "Still Hurting", Beethoven's 3th symphony, "Davy Jones" and "Exile ft. Bon Iver". All these songs, exept the last one, are musical numbers or instrumental numbers, so it makes sense that they have low danceability. "Davy Jones" and Beethoven's 3th are also the ones with the lowest energy (under 0.1). The ones with the lowest valence (under 0.1) are "Fine Line", "No Time To Die", "Falling", "My Future" and again "Once Upon a December". These songs are slow ballet songs. 

Fleur's song with low energy is "i love you" from Billie Eilish. The songs with the lowest danceability (under 0.3) are "Mumzy" and "When We Were Young". 

Hemke's songs with the lowest danceability are "Venus In Furs" and "Answer to Yourself". 

I noticed that here are a few kind of turquoise dots. Those are songs that Hemke and I have in common. They are all songs from Harry styles.

### Looking at the **mode**, **key** and **tempo**
#### How do we like our **mode**? 
```{r}
Spotify_Wrapped$mode_factor <- factor(Spotify_Wrapped$mode, levels = c(1,0), labels = c("Major", "Minor"))

```


```{r}

plotmode<-ggplot(Spotify_Wrapped, aes(x = mode_factor, fill = category)) +
geom_bar(position = position_dodge()) +
labs(x = "Mode", y = "Count") + ggtitle("Major or Minor?")
                                                                                
ggplotly(plotmode)
```

****
Here you can see the mode of the songs in our playlists. We all have more major songs, which I expected. But we still have quite a lot of minor songs, which I didn't expect. 

#### variety of **keys** 

```{r}

plotkeys<-ggplot(Spotify_Wrapped, aes(x = key,fill=category,label=track.name)) +
  geom_histogram() + ggtitle("The Keys of the Songs")
ggplotly(plotkeys)
```

****
This is an overview of the keys of the songs in our playlists. We all have the most songs in C. I found it interesting to see that I have 6 songs in D#/Eb and Fleur and Hemke only 2 each. All three of us have the same amount of songs in A, namely 9 each.

#### How fast do we like our songs? An overview of **tempo**

```{r}
plottempi<-ggplot(Spotify_Wrapped,aes(x = tempo, fill=category, label=track.name)) +
  geom_histogram() + ggtitle("Different Tempi")
ggplotly(plottempi)
```

****
This shows the different tempi of the songs in our playlists. The most songs are around 120BPM. We all have a few very fast songs, but the rest is pretty average. The peak is at 120BPM. 120BPM is what most people prefer. The songs with a fast tempo in my playlist are "Small Talk", "Fired Up", "Vossi Bop" en "Lost In Yesterday". Hemke's two songs are "Small Talk" and "Vossi Bop", the same as two of mine. Fleur's one is "GUESS WHAT? (feat. XakiMichele)"


### But what about **valence**, **energy**, **Loudness** and **speechiness**?
#### **Valence**:positive or negative?
```{r}
Spotify_Wrapped %>%
  ggplot(aes(x = valence)) +
  geom_histogram(binwidth = 0.1) +
  facet_wrap(~category)

```

****
Here you can see the valence of our playlists. How higher the value, how more positive (happy,cheerful) the song is. A low value can be associated with more negative (sad, angry) songs. As you can see we all have a pretty high value, so our songs are positive. I would say Hemke's playlist has the most valence, then Fleur and then Julie. I didn't expect Fleur's playlist to have such high valence, or at least more valence than Julie's. 


#### Are we energetic or not? Let's take a look at the **energy**
```{r}
Julie <- get_playlist_audio_features("","37i9dQZF1EM27HE4pdYYcz") 
Hemke <- get_playlist_audio_features("","37i9dQZF1EMgeVFiZwhb4Z")
Fleur <- get_playlist_audio_features("","37i9dQZF1EMd6LXIgrTylx")

Spotify_Wrapped <-
  bind_rows(
    Julie %>% mutate(category = "Julie"),
    Hemke %>% mutate(category = "Hemke"),
    Fleur %>% mutate(category = "Fleur")
      )

Spotify_Wrapped %>%
  ggplot(aes(x = energy)) +
  geom_histogram(binwidth = 0.1) +
  facet_wrap(~category)


```

****

Fleur's playlist lies between the middle and the higher levels of energy. Hemke's is mostly high. My playlist is a mix of low, middle and high. This is mostly what i expected. I expected Fleur's to have a bit more energy, or at least more energy that Hemke's.

#### **Loudness**

```{r}
Spotify_Wrapped %>%
  ggplot(aes(x = loudness)) +
  geom_histogram() +
  facet_wrap(~category) + ggtitle("Loudness")
```


****
Here you can see the loudness of the playlists. The X-axis shows how much decibels the songs have and the Y-axis how many numbers have those amount of decibels. The lower decible value, the less loud it is (so -20 is less loud than -5). All three are leaning towards the right side of the plot, so all three of us have a pretty loud playlist. Fleur's playlist does have a higher peak in between -10 and -5 decibels.  Hemke's playlist  has a song that is quieter and Fleur as well, but Julie's playlist has more songs in between -20 and -15 decibels.

#### From speaking to singing: **speechiness**

```{r}
Spotify_Wrapped %>%
  ggplot(aes(x = speechiness)) +
  geom_histogram() +
  facet_wrap(~category) + ggtitle("Speechiness")
```

****
Here you can see the speechiness of our playlists. All our songs are bellow the 0.66 value, which means that none or our songs are entirely made of spoken words. This makes sense because it's all music. Fleur's songs are more distributed than Hemke and Julie's. This also makes sense because Fleur listens to more rap, and the values between 0.33 and 0.66 contains music and speech, and thus rap music. Hemke and Julie have a peak below the 0.1 value, and all values below 0.33 represent music. This, again, also makes sense because Hemke and Julie listen more pop music. The few outlier that Hemke and Julie have are rap songs, like "WHAT'S GOOD" and "Vossi Bop". One of Julie's outliers is a song from the musical Hamilton, which is a rap musical. 
What I didn't expect was that the songs Julie has that are completely instrumental (for example "Davy Jones") weren't the ones with the lowest speechiness values. 

### A self similairity matrix and a chordogram: what do they say songs?

####  Chromagram of "Davy Jones" from Pirates of the Caribbean, outlier from **Julie's** playlist.
```{r}
library(compmus)
library(tidyverse)
library(ggplot2)
library(spotifyr)
library(plotly)
library(Cairo)
library(dplyr)

davy <-
  get_tidy_audio_analysis("7klLXey9V4OrC6mwUXf8Fc") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

davy %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```
 
****

This is a chromagram of "Davy jones" from Pirates of the Caribbean. This Song is from Julie's playlist and has the lowest danceability, valence and energy of all the playlists.

#### Self similairity matrix of "Leaders" from Lil Uzi Vert, outlier of **Fleur's** playlist.


```{r}
library(compmus)
library(tidyverse)
library(ggplot2)
library(spotifyr)
library(plotly)
library(Cairo)
library(dplyr)

leaders <-
  get_tidy_audio_analysis("4D7NrSeqkTarBrJ80b2sBc") %>%
  compmus_align(bars, segments) %>%
  select(bars) %>%
  unnest(bars) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
bind_rows(
  leaders %>%
    compmus_self_similarity(pitches, "aitchison") %>%
    mutate(d = d / max(d), type = "Chroma"),
  leaders %>%
    compmus_self_similarity(timbre, "euclidean") %>%
    mutate(d = d / max(d), type = "Timbre")
) %>%
  mutate() %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")




```

****
This is a self similairity matrix of "leaders". This song is from Fleur's playlist and has the most danceability, valence and energy of all the playlists.


### Looking at chords with the help of **chordograms**
#### "Venus in Furs" by The Velvet Underground, outlier of **Hemke's** playlist.



```{r}


circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

venusinfurs <-
  get_tidy_audio_analysis("29engDqjmMr3VLqMm0c0WE") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

venusinfurs %>% 
  compmus_match_pitch_template(
    key_templates,         
    method = "euclidean",  
    norm = "manhattan"     
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")


```

****

This is chordogram of "Venus in Furs", one of the outliers of Hemke's playlist. The darkestcolors of the chords are G#min, C#min, Abmaj and Dbmaj. Around the 100 and 200 seconds this changes. Then it is Emaj and Emin. The song ends with a fade out, so this explains the lighter colors. It has a very clear chord progressions. And you can also hear it in the song very clearly. 

#### "Sunflower, vol 6." by **Harry Styles**
```{r}

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

sun6 <-
  get_tidy_audio_analysis("6iYMfxznTBlcVOgRHab2W0") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

sun6 %>% 
  compmus_match_pitch_template(
    key_templates,         
    method = "euclidean",  
    norm = "manhattan"     
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

**** 
This is a chordogram of "Sunflower, VOl. 6" by Harry Styles. I thought it would be interesting to look at one of Harry styles songs since Hemke and I both listened to his songs. The chords that are used in this song are Cmaj, Fmaj, Cmin and Fmin. Bbmin also gets used but not a lot. The song begins and ends with a fade in/out, which the lighter colors in the beginning and ending of the chordogram show. It's an upbeat song. I didn't expect there to be so little dark colors, so not a very distinctive key or main chord maybe. The Fmaj chord around 125 seconds is a vocal harmony with many layers of vocals. 

#### "i love you" by Billie Eillish, one of **Fleur's** outliers

```{r}

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

iloveyou <-
  get_tidy_audio_analysis("6CcJMwBtXByIz4zQLzFkKc") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

iloveyou %>% 
  compmus_match_pitch_template(
    key_templates,         
    method = "euclidean",  
    norm = "manhattan"     
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")


```

****
This is a chordogram of "i love you" by Billie Eillish, one of the ouliers of Fleur's playlist. It's a very quiet and calm song with only the guitar and a little bit of piano as instruments.The lighter bloks show this. The main chords that are used are Cmaj,Amin Emin and Fmaj. During the darker blok from around 110 seconds to 225 seconds, the second verse starts, which has some background noises, and the chorus and bridge are played, which  have more dynamics that before. 

### **Tempograms**: Studying the tempi
#### "December", **Fleur's** most listened song


```{r}
decemberF <-
  get_tidy_audio_analysis("2XTyx7Txuj6dZ1lLYLj3Wc") %>%
  select(segments) %>%
  unnest(segments)

decemberF %>%
  mutate(loudness_max_time = start + loudness_max_time) %>%
  arrange(loudness_max_time) %>%
  mutate(delta_loudness = loudness_max - lag(loudness_max)) %>%
  ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
  geom_line() +
  xlim(0, 30) +
  theme_minimal() +
  labs(x = "Time (s)", y = "Novelty")

```

****
In this plot you can see the onsets, which is where things change in the song. It has a lot of high peaks. 


```{r}
decemberF <-
  get_tidy_audio_analysis("2XTyx7Txuj6dZ1lLYLj3Wc")

decemberF%>%
tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```



****
Here you can see that "December" has a clear beat. You can see this in the tempogram because of the yellow line. The line sits around 100BPM, which is slower than I expected.

#### "WHAT'S GOOD", one of **Julie's** songs



```{r}
whatsgood <- get_tidy_audio_analysis("6bOkaEXc5CopinGazSLokx")

whatsgood <- get_tidy_audio_analysis("6bOkaEXc5CopinGazSLokx")%>%
   select(segments) %>%
  unnest(segments)

whatsgood %>%
  mutate(loudness_max_time = start + loudness_max_time) %>%
  arrange(loudness_max_time) %>%
  mutate(delta_loudness = loudness_max - lag(loudness_max)) %>%
  ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
  geom_line() +
  xlim(0, 30) +
  theme_minimal() +
  labs(x = "Time (s)", y = "Novelty")


```

****
This plot shows the onsets of the "WHAT's GOOD". It begins with a very high peak, but the rest are a lot smaller.


```{r}
whatsgood <- get_tidy_audio_analysis("6bOkaEXc5CopinGazSLokx")

whatsgood %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

****
"WHAT'S GOOD" has an evident tempo, as you can see from the yellow line in the tempogram. The parts where there is more yellow at the bottom and top, the song has an instrument (I think a synthesizer) that creates a kind of grainy noise. The line sits a bit under 120BPM.

#### "The Less I Know The Better", one of **Hemke's** songs

  
```{r}
tame <- get_tidy_audio_analysis("6K4t31amVTZDgR3sKmwUJJ")

tame <- get_tidy_audio_analysis("6K4t31amVTZDgR3sKmwUJJ")%>%
   select(segments) %>%
  unnest(segments)

tame %>%
  mutate(loudness_max_time = start + loudness_max_time) %>%
  arrange(loudness_max_time) %>%
  mutate(delta_loudness = loudness_max - lag(loudness_max)) %>%
  ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
  geom_line() +
  xlim(0, 30) +
  theme_minimal() +
  labs(x = "Time (s)", y = "Novelty")
```

****
The onsets of "The Less I Know The Better" begins with a very high peak, but the rest is a lot smaller. It looks similair to the onsets of "WHAT'S GOOD".  


```{r}
tame <- get_tidy_audio_analysis("6K4t31amVTZDgR3sKmwUJJ")

tame %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

****
The tempogram is very clear. There is a very obvious yellow line that sits a bit lower that 120BPM. 

### **Clustering**: what fits together?


#### **Fleur**
```{r}
eennaam <-
  get_playlist_audio_features( "6ywNWWhYlG2kr738AAal7D","0Cr3yhXx2WayVeANlooriq") %>%
  add_audio_analysis() %>%
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))
```

```{r}
eennaam_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = eennaam
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  # step_range(all_predictors()) %>% 
  prep(eennaam %>% mutate(track.name = str_trunc(track.name, 20))) %>%
  juice() %>%
  column_to_rownames("track.name")
```

```{r}
eennaam_dist <- dist(eennaam_juice, method = "euclidean")
```


```{r}
eennaam_dist %>% 
  hclust(method = "complete") %>% # Try single, average, and complete.
  dendro_data() %>%
  ggdendrogram()
```

****
This clustering shows the 10 most listened songs of Fleur's playlist. It has two pretty clear clusters. One with only three songs and one with the other seven. The songs on the right side all have a fast tempo and have the same kind of structure.


#### **Hemke**

```{r}
eenaam <-
  get_playlist_audio_features( "0Cr3yhXx2WayVeANlooriq","6ywNWWhYlG2kr738AAal7D") %>%
  add_audio_analysis() %>%
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))
```

```{r}
eenaam_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = eenaam
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  # step_range(all_predictors()) %>% 
  prep(eenaam %>% mutate(track.name = str_trunc(track.name, 20))) %>%
  juice() %>%
  column_to_rownames("track.name")
```

```{r}
eenaam_dist <- dist(eenaam_juice, method = "euclidean")
```

```{r}
eenaam_dist %>% 
  hclust(method = "complete") %>% # Try single, average, and complete.
  dendro_data() %>%
  ggdendrogram()
```

****
This clustering shows the 10 most listened songs of Hemke's. Just like Fleur's, this one also has two pretty clear clusters. But here there are two clusters of five songs. The songs on the right cluster are al a bit slower. The songs in the left have a bit more valence and can be considers happier. 
 

#### **Julie**


```{r}
eeaam <-
  get_playlist_audio_features( "0Cr3yhXx2WayVeANlooriq","018Ld7uIx2xTRPXzVVut8z") %>%
  add_audio_analysis() %>%
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))
```

```{r}
eeaam_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = eeaam
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  # step_range(all_predictors()) %>% 
  prep(eeaam %>% mutate(track.name = str_trunc(track.name, 20))) %>%
  juice() %>%
  column_to_rownames("track.name")
```

```{r}
eeaam_dist <- dist(eeaam_juice, method = "euclidean")
```


```{r}
eeaam_dist %>% 
  hclust(method = "complete") %>% # Try single, average, and complete.
  dendro_data() %>%
  ggdendrogram()
```

****
This clustering shows the 10 most listened songs of Julie's playlist. There are three clear clusters. What I find interesting is that I have a few of the same songs as Hemke, but they are clustered differently. For example, "Golden" and "Lights Up" are together in one cluster here, but in Hemke's cluster "Golden" was on the far left and "Lights Up" on the far right. The middle cluster here has more upbeat songs, and the left cluster has two slow and sad songs. 

### **Classification**:is it the same?  

#### **Nearest Neighbors**
```{r}
juliie <- get_playlist_audio_features("spotify", "37i9dQZF1EM27HE4pdYYcz")
fleuur <- get_playlist_audio_features("spotify", "37i9dQZF1EMd6LXIgrTylx")
hemmke <- get_playlist_audio_features("spotify", "37i9dQZF1EMgeVFiZwhb4Z")
drie <-
  bind_rows(
    juliie %>% mutate(playlist = "Julie") %>% slice_head(n = 20),
    fleuur %>% mutate(playlist = "Fleur") %>% slice_head(n = 20),
    hemmke %>% mutate(playlist = "Hemke") %>% slice_head(n = 20)
  ) 
```


```{r}
drie_features <-
  drie %>%  
  add_audio_analysis() %>% 
  mutate(
    playlist = factor(playlist),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))
```

```{r}
drie_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B +
      c01 + c02 + c03 + c04 + c05 + c06 +
      c07 + c08 + c09 + c10 + c11 + c12,
    data = drie_features,          # Use the same name as the previous block.
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```

```{r}
drie_cv <- drie_features %>% vfold_cv(5)
```


```{r}
knn_model <-
  nearest_neighbor(neighbors = 1) %>%
  set_mode("classification") %>% 
  set_engine("kknn")
drie_knn <- 
  workflow() %>% 
  add_recipe(drie_recipe) %>% 
  add_model(knn_model) %>% 
  fit_resamples(
    drie_cv, 
    control = control_resamples(save_pred = TRUE)
  )
```

```{r}
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)}

drie_knn %>% get_conf_mat()
```

```{r}


drie_knn %>% get_conf_mat() %>% autoplot(type = "heatmap")
```

****
The confusion matrix shows that the classifier can tell Fleur apart from Hemke and Julie, but that it isn't great at telling Hemke and Julie apart from each other. Since most samples from Fleur are classified as Fleur, while most samples from Hemke are classified as Julie and vice versa. This makes sense since Hemke and Julie listen to very similair music. 



#### **Timbre components**
```{r}
drie_features %>%
  ggplot(aes(x = c11, y = c04, colour = playlist, size = energy)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "Timbre Component 11",
    y = "Timbre Component 4",
    size = "Energy",
    colour = "Playlist"
  )
```

****
This plot shows that Hemke and Julie, once again, have a lot of overlap and Fleur is more of a lone wolf. Hemke and Julie have more of the timbre component 11, and Fleur lies a bit in between the two components, with a few outliers to the timbre component 4. 

### **Conclusion**
So, I expected that Hemke and I would have a lot of similar songs in our playlist and that Fleur's playlist would be a lot different. I also thought that Hemke and I would have a lot of valence and that Fleur would have more energy. My expectations were mostly true. Fleur's playlist does have more danceability that Hemke and I's, but Hemke and I also had a good amount of danceablilty. We all three had quite similar amounts of valence in our playlist, but Fleur's had more valence than my playlist. This I didn't expect because I listen to more pop music and I associate that with a positive feeling. I also expected Fleur's playlist to have more energy than it did. All our playlist had approximately the same amount of loudness. Fleur's playlist had a bit more speechiness, which makes sense since she listens to more rap music. Our songs were also tempo wise around the most average tempo, which is 120BPM. I also looked at some of the outliers in our playlists, which was interesting to do. Hemke and I's playlist are indeed more similar to each others than to Fleur's and Fleur's was less different than I thought it would be. 











